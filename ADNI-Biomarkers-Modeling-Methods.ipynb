{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4fe953",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "# Modeling definitions\n",
    " * **Binary Predictions** : For high-level model exploration, standard metrics including AUC of ROC \n",
    "   - Using scipy.--model--.predict\n",
    " * **Continuous Predictions** : Given a model, for feture engineering, Net Reclassification Index (NRI)\n",
    "   - Using scipy.--model--.predict_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b59ef",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Continuous Probabilities per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "779a6248",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def check_cat(prob,thresholds):\n",
    "    cat=0\n",
    "    for i,v in enumerate(thresholds):\n",
    "        if prob>v:\n",
    "            cat=i\n",
    "    return cat\n",
    "\n",
    "def make_cat_matrix(ref, new, indices, thresholds):\n",
    "    num_cats=len(thresholds)\n",
    "    mat=np.zeros((num_cats,num_cats))\n",
    "    for i in indices:\n",
    "        row,col=check_cat(ref[i],thresholds),check_cat(new[i],thresholds)\n",
    "        mat[row,col]+=1\n",
    "    return mat\n",
    "        \n",
    "def nri(y_truth,y_ref, y_new,risk_thresholds):\n",
    "    event_index = np.where(y_truth==1)[0]\n",
    "    nonevent_index = np.where(y_truth==0)[0]\n",
    "    event_mat=make_cat_matrix(y_ref,y_new,event_index,risk_thresholds)\n",
    "    nonevent_mat=make_cat_matrix(y_ref,y_new,nonevent_index,risk_thresholds)\n",
    "#     events_up, events_down = event_mat[0,1:].sum()+event_mat[1,2:].sum()+event_mat[2,3:].sum(),event_mat[1,:1].sum()+event_mat[2,:2].sum()+event_mat[3,:3].sum()\n",
    "#     nonevents_up, nonevents_down = nonevent_mat[0,1:].sum()+nonevent_mat[1,2:].sum()+nonevent_mat[2,3:].sum(),nonevent_mat[1,:1].sum()+nonevent_mat[2,:2].sum()+nonevent_mat[3,:3].sum()\n",
    "    events_up, events_down = event_mat[0,1],event_mat[1,0]\n",
    "    nonevents_up, nonevents_down = nonevent_mat[0,1],nonevent_mat[1,0]\n",
    "    nri_events = (events_up/len(event_index))-(events_down/len(event_index))\n",
    "    nri_nonevents = (nonevents_down/len(nonevent_index))-(nonevents_up/len(nonevent_index))\n",
    "    return nri_events, nri_nonevents, nri_events + nri_nonevents \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede54afd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Binary predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2040d7d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "import compare_auc_delong_xu\n",
    "from scipy import stats\n",
    "\n",
    "def get_auc_ci(auc, auc_cov, alpha=0.95) :\n",
    "    alpha = .95\n",
    "    auc_std = np.sqrt(auc_cov)\n",
    "    lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "    ci = stats.norm.ppf(\n",
    "        lower_upper_q,\n",
    "        loc=auc,\n",
    "        scale=auc_std)\n",
    "\n",
    "    ci[ci > 1] = 1\n",
    "    return ci\n",
    "  \n",
    "def cv_scorer(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "    sen = (tp)/(tp+fn)\n",
    "    sp = (tn)/(tn+fp)\n",
    "    ppv = (tp)/(tp+fp)\n",
    "    npv = (tn)/(tn+fn)\n",
    "    f1 = 2*(sen*ppv)/(sen+ppv)\n",
    "    auc = round(roc_auc_score(y, y_pred), 3)\n",
    "\n",
    "    return {\n",
    "                'acc': round(acc, 3)*100,\n",
    "                'sen' : round(sen, 3)*100,\n",
    "                'spe' : round(sp, 3)*100,\n",
    "                'ppv' : round(ppv, 3)*100,\n",
    "                'npv' :  round(npv, 3)*100,\n",
    "                'auc' : round(auc, 2), \n",
    "                'nri' : 1\n",
    "    }  \n",
    "\n",
    "def add_mean_metrics(scorer) :\n",
    "    metrics = ['acc', 'sen', 'spe', 'ppv', 'npv', 'nri']\n",
    "    for m in metrics :\n",
    "        scorer[m] = round(scorer[\"test_\"+m].mean(), 1)\n",
    "        scorer[m+\"_err\"] = round((scorer[\"test_\"+m].std())/np.sqrt(scorer[\"test_\"+m].size), 2)\n",
    "    m = 'auc'\n",
    "    scorer[m] = round(scorer[\"test_\"+m].mean(), 2)\n",
    "    scorer[m+\"_err\"] = round((scorer[\"test_\"+m].std())/np.sqrt(scorer[\"test_\"+m].size), 2)\n",
    "    return scorer\n",
    "\n",
    "def extract_metrics(y, y_pred, y_pred_prob=None, y_pred_prob_ref=None):\n",
    "#     tp, fn, fp, tn = confusion_matrix(y, y_pred).ravel()\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "#     if debug: print('TN : {}, FN : {}, FP : {}, TP : {}'.format(tn, fp, fn, tp))\n",
    "    acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "    sen = (tp)/(tp+fn)\n",
    "    sp = (tn)/(tn+fp)\n",
    "    ppv = (tp)/(tp+fp)\n",
    "    npv = (tn)/(tn+fn)\n",
    "    f1 = 2*(sen*ppv)/(sen+ppv)\n",
    "#    fpr = (fp)/(fp+tn)\n",
    "#    tpr = (tp)/(tp+fn)\n",
    "    auc = round(roc_auc_score(y, y_pred), 3)\n",
    "    auc_delong, auc_cov = compare_auc_delong_xu.delong_roc_variance(y, y_pred)\n",
    "    auc_std = np.sqrt(auc_cov)\n",
    "    auc_ci = get_auc_ci(auc_delong, auc_cov)\n",
    "    bp = (tp + fn) / (tp+tn+fp+fn)\n",
    "    \n",
    "    if y_pred_prob is None or y_pred_prob_ref is None :\n",
    "        ev, nonev, tot = 1, 1, 2\n",
    "    else :\n",
    "        \n",
    "        ev, nonev, tot = nri(y,y_pred_prob_ref[:,1],y_pred_prob[:,1],[0.02,0.5])\n",
    "\n",
    "    return {\n",
    "                'acc': round(acc, 3)*100,\n",
    "                'sen' : round(sen, 3)*100,\n",
    "                'spe' : round(sp, 3)*100,\n",
    "                'ppv' : round(ppv, 3)*100,\n",
    "                'npv' :  round(npv, 3)*100,\n",
    "                'auc' : round(auc, 2), \n",
    "#                 'AUC' : round(auc_delong, 2), \n",
    "                'auc_err' : round(auc_std, 2),\n",
    "                'nri' : round(tot, 2), \n",
    "                # 'NRI (E)' : ev, \n",
    "                # 'NRI (NE)' : nonev, \n",
    "#         '{}[{}]'.format(auc_delong, str(auc_ci)),\n",
    "#                 'AUC' : auc,\n",
    "                'bp' : round(bp, 3)*100\n",
    "        \n",
    "    }  \n",
    "# ['Accuracy(%)', 'Sensitivity(%)', 'Specificity(%)', 'PPV(%)', 'NPV(%)', 'AUC','AUC(err)','NRI','Base prevalance(%)'] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83514f4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metrics Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2bb42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverallMetrics:\n",
    "    def __init__(self):\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "    def getDF(self) :\n",
    "        return self.df\n",
    "    \n",
    "    def addMetrics(self, metrics_record, index_values):\n",
    "        for idx in index_values :\n",
    "            metrics_record[idx] = index_values[idx]\n",
    "        metrics = pd.DataFrame.from_records([metrics_record])#, \n",
    "                                        # index=[[key] for key in keys])\n",
    "        self.df  = pd.concat([self.df, metrics])\n",
    "    \n",
    "class MetricsByFeatureAddition:\n",
    "    def __init__(self):\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "    def getDF(self) :\n",
    "        return self.df\n",
    "    \n",
    "    def addMetrics(self, metrics_record, index_values):\n",
    "        metrics_df = pd.DataFrame.from_records([metrics_record])\n",
    "        _by_metric = pd.Series(metrics_df.mean(axis=0), name='ALL').reset_index(name='value').rename(columns = {'index' : 'metric'})\n",
    "        for idx in index_values :\n",
    "            _by_metric[idx] = index_values[idx]\n",
    "        self.df  = pd.concat([self.df, _by_metric])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada97283",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Encoding category fields**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b10ac02",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ord_enc = OrdinalEncoder()\n",
    "\n",
    "def encodeCatsAndNormalize(df) :\n",
    "    encodeFields = [\"PTGENDER\", \"ETHNICRACE\", \n",
    "                   #\"PTETHCAT\", \"PTRACCAT\",\n",
    "                   \"PTMARRY\"]\n",
    "    df_fields = df.columns\n",
    "    for ef in encodeFields :\n",
    "        if ef in df_fields:\n",
    "            df[ef] = ord_enc.fit_transform(df[[ef]])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f7611",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train and Test - One Model\n",
    "\n",
    " - Training : CD+AD\n",
    "     - Metrics : fit and predict on training data\n",
    " - Prediction : MCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe477216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(clf, X, y, clf_ref=None) :\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=123)        \n",
    "    clf.fit(x_train, y_train)\n",
    "#     clf.fit(normed_data, y)\n",
    "    print('OOB Score: ', clf.oob_score_)\n",
    "    y_pred = clf.predict(x_test)\n",
    "#     y_pred = clf.predict(normed_data)\n",
    "    if clf_ref is not None:\n",
    "        test_pred_prob=clf.predict_proba(x_test[curr_model_features])\n",
    "    #             test_pred_prob=clf.predict_proba(normed_data[curr_model_features])\n",
    "        clf_ref = outputs_by_model[-1][\"model\"] #outputs_by_model[i-1]\n",
    "        test_pred_prob_ref=clf_ref.predict_proba(x_test[ref_model_features])\n",
    "        return extract_metrics(y_test, y_pred, test_pred_prob, test_pred_prob_ref)\n",
    "    else :\n",
    "        return extract_metrics(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fa7e8",
   "metadata": {},
   "source": [
    "## Model training and testing - ALL Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "57593ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainandtest(supertrain, feature_cols, train_ycol, model_label, ALLMETRICS, model_type='ALL') :\n",
    "    print(feature_cols)\n",
    "    training_metrics = ALLMETRICS[\"train\"][\"all\"]\n",
    "    metrics_by_feature_addition = ALLMETRICS[\"train\"][\"feature_addition\"]\n",
    "    final_metrics_ = ALLMETRICS[\"test\"][\"all\"]\n",
    "    MCI_metrics_by_feature_addition = ALLMETRICS[\"test\"][\"feature_addition\"]\n",
    "    \n",
    "    ###TRAIN\n",
    "    train_features = feature_cols + [train_ycol]\n",
    "    adm = supertrain[train_features].dropna()\n",
    "    adm = encodeCatsAndNormalize(adm)\n",
    "\n",
    "    N_train = adm.shape[0]\n",
    "    X = adm[feature_cols]\n",
    "    y = adm[train_ycol]\n",
    "\n",
    "    # Transform data\n",
    "    sc = StandardScaler()\n",
    "    normed_data = pd.DataFrame(sc.fit_transform(X), columns = X.columns)\n",
    "\n",
    "    clf=RandomForestClassifier(oob_score=True)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(normed_data, y, test_size=0.30, random_state=123)        \n",
    "#     clf.fit(x_train, y_train)\n",
    "# #     clf.fit(normed_data, y)\n",
    "#     print('OOB Score: ', clf.oob_score_)cv\n",
    "#     y_pred = clf.predict(x_test)\n",
    "#     y_pred = clf.predict(normed_data)\n",
    "    \n",
    "#     training_metrics.addMetrics(extract_metrics(y_test, y_pred),\n",
    "    training_metrics.addMetrics(fit_and_predict(clf, normed_data, y),\n",
    "#     training_metrics.addMetrics(extract_metrics(y, y_pred),\n",
    "                                {'model' : model_label,  \n",
    "                                 'feature': model_type, \n",
    "                                 'count': '{}({}%)'.format(str(N_train), str(round(100 * y.sum()/y.shape[0], 2)))}\n",
    "                               )\n",
    "\n",
    "    clf=RandomForestClassifier(oob_score=True)\n",
    "    clf.fit(normed_data, y)\n",
    "    feature_imp = pd.Series(clf.feature_importances_, index=X.columns, name=model_label).sort_values(ascending=False)\n",
    "#     metrics_by_feature_addition.addMetrics(extract_metrics(y, y_pred),\n",
    "#     metrics_by_feature_addition.addMetrics(extract_metrics(y_test, y_pred),\n",
    "    metrics_by_feature_addition.addMetrics(fit_and_predict(clf, normed_data, y),\n",
    "                      {'model' : model_label,\n",
    "                      'feature' : model_type,\n",
    "                      'feature_weight' : 1})\n",
    "    model_name = '{}, N (%AD)= {}({}%)'.format(model_label, str(N_train), str(round(100 * y.sum()/y.shape[0], 2)))\n",
    "\n",
    "    ### Prediction on MCI\n",
    "    test_features = feature_cols + [test_ycol]\n",
    "    mci_datasets = [{'key' : 'ALL', 'data' : ADM_MCI[test_features].dropna()}, \n",
    "                    {'key' : 'AB+', 'data' : ADM_MCI[ADM_MCI[\"ABP\"] == 1][test_features].dropna()},\n",
    "                    {'key' : 'AB-', 'data' : ADM_MCI[ADM_MCI[\"ABP\"] == 0][test_features].dropna()},\n",
    "                   ]  \n",
    "    for mci_dataset in mci_datasets:\n",
    "        mci_df = mci_dataset['data']\n",
    "        mci_test_X = mci_df[feature_cols]\n",
    "        mci_test_X = encodeCatsAndNormalize(mci_test_X)\n",
    "        MCI_abp_true = mci_df[test_ycol]\n",
    "        N = mci_df[test_ycol].shape[0]\n",
    "        BP = 100 * (mci_df[mci_df[test_ycol] == 1].shape[0])/N\n",
    "        # Transform data\n",
    "        sc_mci = StandardScaler()\n",
    "        normed_data_mci = pd.DataFrame(sc_mci.fit_transform(mci_test_X), columns = mci_test_X.columns)\n",
    "        y_abp_pred = clf.predict(normed_data_mci)\n",
    "        final_metrics_.addMetrics(extract_metrics(MCI_abp_true, y_abp_pred, True),\n",
    "                                {'dataset': mci_dataset['key'], 'model' : model_label,  'feature': model_type, 'count': '{}'.format(N)}\n",
    "                                   )\n",
    "\n",
    "        MCI_metrics_by_feature_addition.addMetrics(extract_metrics(MCI_abp_true, y_abp_pred),\n",
    "                      {'model' : model_label,\n",
    "                      'feature' : model_type,\n",
    "                      'feature_weight' : 1,\n",
    "                      'dataset' : mci_dataset['key']})\n",
    "    \n",
    "    ALLMETRICS = {\"train\" : {\n",
    "                    \"all\" : training_metrics,\n",
    "                    \"feature_addition\" : metrics_by_feature_addition\n",
    "                 },\n",
    "                \"test\": {\n",
    "                    \"all\" : final_metrics_,\n",
    "                    \"feature_addition\" : MCI_metrics_by_feature_addition\n",
    "                 }\n",
    "            }\n",
    "\n",
    "    return clf, feature_imp, ALLMETRICS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162ed3e",
   "metadata": {},
   "source": [
    "## Model training and testing - ALL Features - CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f62e3e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "class TunedRF:\n",
    "    def __init__(self, scoring):\n",
    "        self.model = RandomForestClassifier(random_state= 42)\n",
    "        self.cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "        self.grid = dict()\n",
    "        self.grid['n_estimators'] = np.array([50, 100, 150, 200, 250, 300, 350])\n",
    "        self.search = GridSearchCV(RandomForestClassifier(random_state= 42), self.grid, scoring=scoring, \n",
    "                                  refit=\"auc\", \n",
    "                                  return_train_score=True,\n",
    "                              cv=self.cv, n_jobs=-1)\n",
    "    \n",
    "    def getGSObj(self):\n",
    "        return self.search\n",
    "    \n",
    "    def getBestEstimatorMetrics(self) :\n",
    "        metrics = pd.DataFrame(self.search.cv_results_).loc[self.search.best_index_]\n",
    "        mdf = {}\n",
    "        metrics_of_interest = ['acc', 'sen', 'spe', 'ppv', 'npv', 'auc', 'nri']\n",
    "        for moi in metrics_of_interest:\n",
    "            mdf[moi] = metrics['mean_test_'+moi]\n",
    "            mdf[moi+'_err'] = metrics['std_test_'+moi]\n",
    "        return mdf\n",
    "        \n",
    "    def getFeatureImportances(self, X, y) :\n",
    "        print('Feature importance by permutation...')\n",
    "        skf = StratifiedKFold(n_splits=5)\n",
    "        fis = []\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            clf = self.getTunedModel()\n",
    "            X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            clf.fit(X_train, y_train)\n",
    "            r = permutation_importance(clf, X_test, y_test,\n",
    "                           n_repeats=30,\n",
    "                           random_state=0, scoring='roc_auc')\n",
    "            fis.append(r.importances_mean)\n",
    "        return np.array(fis).mean(axis=0)\n",
    "            \n",
    "    def getBestFitModel(self, X, y) :\n",
    "        self.search.fit(X, y)\n",
    "#         print('Config of Tuned RF : %s' % self.search.best_params_)\n",
    "        return self.search.best_estimator_, self.getBestEstimatorMetrics() \n",
    "    \n",
    "    def getTunedModel(self) :\n",
    "        return RandomForestClassifier(random_state= 42, \\\n",
    "                                      n_estimators=self.search.best_params_['n_estimators'])\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "def getFeatureImportances(X, y) :\n",
    "    print('Feature importance by permutation...')\n",
    "    skf = StratifiedKFold(n_splits=5)#, n_repeats=5)\n",
    "    fis = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        clf = RandomForestClassifier(oob_score=True)\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        r = permutation_importance(clf, X_test, y_test,\n",
    "                       n_repeats=30,\n",
    "                       random_state=123, scoring='roc_auc')\n",
    "        fis.append(r.importances_mean)\n",
    "    return np.array(fis).mean(axis=0)\n",
    "\n",
    "def trainandtest_CV(supertrain, feature_cols, train_ycol, model_label, ALLMETRICS, model_type='ALL', ft_eng=True) :\n",
    "    print(feature_cols)\n",
    "    training_metrics = ALLMETRICS[\"train\"][\"all\"]\n",
    "    metrics_by_feature_addition = ALLMETRICS[\"train\"][\"feature_addition\"]\n",
    "    final_metrics_ = ALLMETRICS[\"test\"][\"all\"]\n",
    "    MCI_metrics_by_feature_addition = ALLMETRICS[\"test\"][\"feature_addition\"]\n",
    "    \n",
    "    ###TRAIN\n",
    "    train_features = feature_cols + [train_ycol]\n",
    "    adm = supertrain[train_features].dropna()\n",
    "    adm = encodeCatsAndNormalize(adm)\n",
    "\n",
    "    # Shuffle the dataset\n",
    "#     adm = adm.sample(frac = 1).reset_index().drop(['index'], axis=1)\n",
    "    N_train = adm.shape[0]\n",
    "    X = adm[feature_cols]\n",
    "    y = adm[train_ycol]\n",
    "\n",
    "    # Transform data\n",
    "    sc = StandardScaler()\n",
    "    normed_data = pd.DataFrame(sc.fit_transform(X), columns = X.columns)\n",
    "\n",
    "    clf=RandomForestClassifier(oob_score=True)\n",
    "#     clf = TunedRF(cv_scorer)\n",
    "#     best_fit_model, metrics = clf.getBestFitModel(normed_data, y)\n",
    "#     tuned_clf = clf.getTunedModel()\n",
    "    cv_results = cross_validate(clf, X, y, cv=5,\n",
    "                             scoring=cv_scorer\n",
    "#                                 , return_estimator=True\n",
    "                               )\n",
    "#     getFeatureImportances(normed_data, y)\n",
    "    validation_metrics = add_mean_metrics(cv_results) \n",
    "    feature_imp = None\n",
    "    if ft_eng :\n",
    "        mean_ft_imp = getFeatureImportances(normed_data, y) \n",
    "        feature_imp = pd.Series(mean_ft_imp, index=X.columns, name=model_label).sort_values(ascending=False)\n",
    "    #mean_ft_imp = np.array([c.feature_importances_ for c in cv_results['estimator']]).mean(axis=0)\n",
    "    training_metrics.addMetrics(validation_metrics,\n",
    "                                {'model' : model_label,  \n",
    "                                 'feature': model_type, \n",
    "                                 'count': '{}({}%)'.format(str(N_train), str(round(100 * y.sum()/y.shape[0], 2)))}\n",
    "                               )\n",
    "\n",
    "    metrics_by_feature_addition.addMetrics(validation_metrics,\n",
    "                      {'model' : model_label,\n",
    "                      'feature' : model_type,\n",
    "                      'feature_weight' : 1})\n",
    "    \n",
    "    clf.fit(normed_data, y)\n",
    "#     model_ft_imps = clf.getFeatureImportances(normed_data, y)\n",
    "#     feature_imp = pd.Series(clf.feature_importances_, index=X.columns, name=model_label).sort_values(ascending=False)\n",
    "#     feature_imp = pd.Series(model_ft_imps, index=X.columns, name=model_label).sort_values(ascending=False)\n",
    "    model_name = '{}, N (%AD)= {}({}%)'.format(model_label, str(N_train), str(round(100 * y.sum()/y.shape[0], 2)))\n",
    "\n",
    "    ### Prediction on MCI\n",
    "    test_features = feature_cols + [test_ycol]\n",
    "    mci_datasets = [{'key' : 'ALL', 'data' : ADM_MCI[test_features].dropna()}, \n",
    "                    {'key' : 'AB+', 'data' : ADM_MCI[ADM_MCI[\"ABP\"] == 1][test_features].dropna()},\n",
    "                    {'key' : 'AB-', 'data' : ADM_MCI[ADM_MCI[\"ABP\"] == 0][test_features].dropna()},\n",
    "                   ]  \n",
    "    for mci_dataset in mci_datasets:\n",
    "        mci_df = mci_dataset['data']\n",
    "        mci_test_X = mci_df[feature_cols]\n",
    "        mci_test_X = encodeCatsAndNormalize(mci_test_X)\n",
    "        MCI_abp_true = mci_df[test_ycol]\n",
    "        N = mci_df[test_ycol].shape[0]\n",
    "        BP = 100 * (mci_df[mci_df[test_ycol] == 1].shape[0])/N\n",
    "        # Transform data\n",
    "        sc_mci = StandardScaler()\n",
    "        normed_data_mci = pd.DataFrame(sc_mci.fit_transform(mci_test_X), columns = mci_test_X.columns)\n",
    "        y_abp_pred = clf.predict(normed_data_mci)\n",
    "        final_metrics_.addMetrics(extract_metrics(MCI_abp_true, y_abp_pred, True),\n",
    "                                {'dataset': mci_dataset['key'], 'model' : model_label,  'feature': model_type, 'count': '{}'.format(N)}\n",
    "                                   )\n",
    "\n",
    "        MCI_metrics_by_feature_addition.addMetrics(extract_metrics(MCI_abp_true, y_abp_pred),\n",
    "                      {'model' : model_label,\n",
    "                      'feature' : model_type,\n",
    "                      'feature_weight' : 1,\n",
    "                      'dataset' : mci_dataset['key']})\n",
    "    \n",
    "    ALLMETRICS = {\"train\" : {\n",
    "                    \"all\" : training_metrics,\n",
    "                    \"feature_addition\" : metrics_by_feature_addition\n",
    "                 },\n",
    "                \"test\": {\n",
    "                    \"all\" : final_metrics_,\n",
    "                    \"feature_addition\" : MCI_metrics_by_feature_addition\n",
    "                 }\n",
    "            }\n",
    "\n",
    "    return clf, ALLMETRICS, feature_imp \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc7651",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "025113cc",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def feature_engineer(supertrain, feature_cols, train_ycol, model_label, ALLMETRICS) :\n",
    "    metrics_by_feature_addition = ALLMETRICS[\"train\"][\"feature_addition\"]\n",
    "    MCI_metrics_by_feature_addition = ALLMETRICS[\"test\"][\"feature_addition\"]\n",
    "    \n",
    "    ###TRAIN\n",
    "    train_features = feature_cols + [train_ycol]\n",
    "    adm = supertrain[train_features].dropna()\n",
    "    adm = encodeCatsAndNormalize(adm)\n",
    "    # Shuffle the dataset\n",
    "    adm = adm.sample(frac = 1, random_state=123).reset_index().drop(['index'], axis=1)\n",
    "    N_train = adm.shape[0]\n",
    "    X = adm[feature_cols]\n",
    "    y = adm[train_ycol]\n",
    "    # Transform data\n",
    "    sc = StandardScaler()\n",
    "    normed_data = pd.DataFrame(sc.fit_transform(X), columns = X.columns)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(normed_data, y, test_size=0.30, random_state=123)        \n",
    "\n",
    "    fields_by_weight = feature_imp.index\n",
    "    outputs_by_model = []\n",
    "    print('Feature engineering : ', end='')\n",
    "    fields_final_model = []\n",
    "    validation_metrics_ref = None\n",
    "    for i in range(len(fields_by_weight)) :\n",
    "        print(fields_by_weight[i]+', ', end='')\n",
    "        ref_model_features = []\n",
    "        if len(outputs_by_model) > 0:\n",
    "            ref_model_features = outputs_by_model[-1][\"model_features\"]\n",
    "        curr_model_features = ref_model_features + [fields_by_weight[i]]\n",
    "            \n",
    "        incremental_fields = fields_by_weight[:i+1]\n",
    "        inc_field = fields_by_weight[i]\n",
    "        feature_weight = round(feature_imp.loc[inc_field], 3)\n",
    "        features_label = ('' if i == 0 else '+') + inc_field \n",
    "\n",
    "        clf=RandomForestClassifier()\n",
    "        count = 1\n",
    "        clf.fit(x_train[curr_model_features], y_train)\n",
    "#         clf.fit(normed_data[curr_model_features], y)\n",
    "        y_pred = clf.predict(x_test[curr_model_features])\n",
    "        # metrics = pd.DataFrame.from_records([extract_metrics(y_test, y_pred)], index=[count])\n",
    "        test_pred_prob, test_pred_prob_ref = None, None\n",
    "        # saveMetrics = False\n",
    "        ### For NRI\n",
    "        if i > 0 :\n",
    "            ### NRI\n",
    "            test_pred_prob=clf.predict_proba(x_test[curr_model_features])\n",
    "#             test_pred_prob=clf.predict_proba(normed_data[curr_model_features])\n",
    "            clf_ref = outputs_by_model[-1][\"model\"] #outputs_by_model[i-1]\n",
    "            test_pred_prob_ref=clf_ref.predict_proba(x_test[ref_model_features])\n",
    "#             test_pred_prob_ref=clf_ref.predict_proba(normed_data[ref_model_features])\n",
    "\n",
    "#         validation_metrics = extract_metrics(y_test, y_pred, test_pred_prob, test_pred_prob_ref)\n",
    "        validation_metrics = extract_metrics(y_test, y_pred, test_pred_prob, test_pred_prob_ref)\n",
    "        addFeature = (i > 0 and (validation_metrics[\"auc\"] - validation_metrics_ref[\"auc\"] >= 0.01) )\n",
    "#         if validation_metrics[\"NRI\"] > 0 :\n",
    "        if i==0 or addFeature :\n",
    "            test_features = feature_cols + [test_ycol]\n",
    "            mci_datasets = [{'key' : 'ALL', 'data' : ADM_MCI[test_features].dropna()}, \n",
    "                            {'key' : 'AB+', 'data' : ADM_MCI[ADM_MCI[\"ABP\"] == 1][test_features].dropna()},\n",
    "                            {'key' : 'AB-', 'data' : ADM_MCI[ADM_MCI[\"ABP\"] == 0][test_features].dropna()},\n",
    "                           ]  \n",
    "            for mci_dataset in mci_datasets:\n",
    "                mci_df = mci_dataset['data']\n",
    "                mci_test_X = mci_df[feature_cols]\n",
    "                mci_test_X = encodeCatsAndNormalize(mci_test_X)\n",
    "                MCI_abp_true = mci_df[test_ycol]\n",
    "                N = mci_df[test_ycol].shape[0]\n",
    "                BP = 100 * (mci_df[mci_df[test_ycol] == 1].shape[0])/N\n",
    "                # Transform data\n",
    "                sc_mci = StandardScaler()\n",
    "                normed_data_mci = pd.DataFrame(sc_mci.fit_transform(mci_test_X), columns = mci_test_X.columns)\n",
    "                ###MCI Test\n",
    "                y_abp_pred = clf.predict(normed_data_mci[curr_model_features])\n",
    "                mci_test_pred_prob, mci_test_pred_prob_ref = None, None\n",
    "                if i > 0 :\n",
    "                    ### NRI\n",
    "                    mci_test_pred_prob=clf.predict_proba(normed_data_mci[curr_model_features])\n",
    "                    clf_ref = outputs_by_model[-1][\"model\"] #outputs_by_model[i-1]\n",
    "                    mci_test_pred_prob_ref=clf_ref.predict_proba(normed_data_mci[ref_model_features])\n",
    "                # metrics_mci = pd.DataFrame.from_records([extract_metrics(MCI_abp_true, y_abp_pred)], index=[count])\n",
    "\n",
    "                ###MCI\n",
    "                MCI_metrics_by_feature_addition.addMetrics(\n",
    "                    extract_metrics(MCI_abp_true, y_abp_pred, mci_test_pred_prob, mci_test_pred_prob_ref), #Add NRI flag\n",
    "                      {'model' : model_label,\n",
    "                      'feature' : features_label,\n",
    "                      'feature_weight' : feature_weight,\n",
    "                      'dataset' : mci_dataset['key']})\n",
    "            fields_final_model.append(fields_by_weight[i])\n",
    "            outputs_by_model.append({\"model\" : clf, \"model_features\" : curr_model_features})\n",
    "            metrics_by_feature_addition.addMetrics(validation_metrics, #Add NRI flag\n",
    "                      {'model' : model_label,\n",
    "                      'feature' : features_label,\n",
    "                      'feature_weight' : feature_weight})\n",
    "            validation_metrics_ref = validation_metrics\n",
    "            \n",
    "    ALLMETRICS[\"train\"][\"feature_addition\"] = metrics_by_feature_addition\n",
    "    ALLMETRICS[\"test\"][\"feature_addition\"] = MCI_metrics_by_feature_addition\n",
    "\n",
    "    return fields_final_model, ALLMETRICS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7243a",
   "metadata": {},
   "source": [
    "## Feature Engineering - CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b68af422",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def feature_engineer_CV(supertrain, feature_cols, train_ycol, model_label, ALLMETRICS) :\n",
    "    metrics_by_feature_addition = ALLMETRICS[\"train\"][\"feature_addition\"]\n",
    "    MCI_metrics_by_feature_addition = ALLMETRICS[\"test\"][\"feature_addition\"]\n",
    "    \n",
    "    ###TRAIN\n",
    "    train_features = feature_cols + [train_ycol]\n",
    "    adm = supertrain[train_features].dropna()\n",
    "    adm = encodeCatsAndNormalize(adm)\n",
    "    # Shuffle the dataset\n",
    "#     adm = adm.sample(frac = 1, random_state=123).reset_index().drop(['index'], axis=1)\n",
    "    N_train = adm.shape[0]\n",
    "    X = adm[feature_cols]\n",
    "    y = adm[train_ycol]\n",
    "    # Transform data\n",
    "    sc = StandardScaler()\n",
    "    normed_data = pd.DataFrame(sc.fit_transform(X), columns = X.columns)\n",
    "\n",
    "    fields_by_weight = feature_imp.index\n",
    "    outputs_by_model = []\n",
    "    print('Feature engineering : ', end='')\n",
    "    fields_final_model = []\n",
    "    validation_metrics_ref = None\n",
    "    for i in range(len(fields_by_weight)) :\n",
    "        print(fields_by_weight[i]+', ', end='')\n",
    "        ref_model_features = []\n",
    "        if len(outputs_by_model) > 0:\n",
    "            ref_model_features = outputs_by_model[-1][\"model_features\"]\n",
    "        curr_model_features = ref_model_features + [fields_by_weight[i]]\n",
    "            \n",
    "        incremental_fields = fields_by_weight[:i+1]\n",
    "        inc_field = fields_by_weight[i]\n",
    "        feature_weight = round(feature_imp.loc[inc_field], 3)\n",
    "        features_label = ('' if i == 0 else '+') + inc_field \n",
    "\n",
    "        count = 1\n",
    "#         clf = TunedRF(cv_scorer)\n",
    "#         best_fit_model, metrics = clf.getBestFitModel(normed_data[curr_model_features], y)\n",
    "#         tuned_clf = clf.getTunedModel()\n",
    "        clf=RandomForestClassifier(oob_score=True)\n",
    "        cv_results = cross_validate(clf, normed_data[curr_model_features], y, cv=5,\n",
    "                                 scoring=cv_scorer)\n",
    "        validation_metrics = add_mean_metrics(cv_results)\n",
    "        addFeature = (i > 0 and (validation_metrics[\"auc\"] - validation_metrics_ref[\"auc\"] >= 0.01) )\n",
    "        if i==0 or addFeature :\n",
    "            clf=RandomForestClassifier(oob_score=True)\n",
    "            clf.fit(normed_data[curr_model_features], y)\n",
    "            test_features = feature_cols + [test_ycol]\n",
    "            mci_datasets = [{'key' : 'ALL', 'data' : ADM_MCI[test_features].dropna()}, \n",
    "                            {'key' : 'AB+', 'data' : ADM_MCI[ADM_MCI[\"ABP\"] == 1][test_features].dropna()},\n",
    "                            {'key' : 'AB-', 'data' : ADM_MCI[ADM_MCI[\"ABP\"] == 0][test_features].dropna()},\n",
    "                           ]  \n",
    "            for mci_dataset in mci_datasets:\n",
    "                mci_df = mci_dataset['data']\n",
    "                mci_test_X = mci_df[feature_cols]\n",
    "                mci_test_X = encodeCatsAndNormalize(mci_test_X)\n",
    "                MCI_abp_true = mci_df[test_ycol]\n",
    "                N = mci_df[test_ycol].shape[0]\n",
    "                BP = 100 * (mci_df[mci_df[test_ycol] == 1].shape[0])/N\n",
    "                # Transform data\n",
    "                sc_mci = StandardScaler()\n",
    "                normed_data_mci = pd.DataFrame(sc_mci.fit_transform(mci_test_X), columns = mci_test_X.columns)\n",
    "                ###MCI Test\n",
    "                y_abp_pred = clf.predict(normed_data_mci[curr_model_features])\n",
    "                ###MCI\n",
    "                MCI_metrics_by_feature_addition.addMetrics(\n",
    "                    extract_metrics(MCI_abp_true, y_abp_pred),\n",
    "                      {'model' : model_label,\n",
    "                      'feature' : features_label,\n",
    "                      'feature_weight' : feature_weight,\n",
    "                      'dataset' : mci_dataset['key']})\n",
    "            fields_final_model.append(fields_by_weight[i])\n",
    "            outputs_by_model.append({\"model\" : clf, \"model_features\" : curr_model_features})\n",
    "            metrics_by_feature_addition.addMetrics(validation_metrics,\n",
    "                      {'model' : model_label,\n",
    "                      'feature' : features_label,\n",
    "                      'feature_weight' : feature_weight})\n",
    "            validation_metrics_ref = validation_metrics\n",
    "            \n",
    "    ALLMETRICS[\"train\"][\"feature_addition\"] = metrics_by_feature_addition\n",
    "    ALLMETRICS[\"test\"][\"feature_addition\"] = MCI_metrics_by_feature_addition\n",
    "\n",
    "    return fields_final_model, ALLMETRICS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0caecc",
   "metadata": {},
   "source": [
    "## McNemar test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff459c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import mcnemar_table, mcnemar\n",
    "\n",
    "def mcnemar_test(supertrain, feature_cols, final_features, train_ycol, test_ycol) :\n",
    "    print(feature_cols)\n",
    "    ###TRAIN\n",
    "    train_features = feature_cols + [train_ycol]\n",
    "    adm_full = supertrain[train_features].dropna()\n",
    "    adm = encodeCatsAndNormalize(adm_full)\n",
    "    # Transform data\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    X_full = adm_full[feature_cols]\n",
    "    X_final = adm_full[final_features]\n",
    "    y = adm[train_ycol]\n",
    "\n",
    "    normed_data_full = pd.DataFrame(sc.fit_transform(X_full), columns = X_full.columns)\n",
    "    normed_data_final = normed_data_full[final_features]\n",
    "    \n",
    "    clf_full=RandomForestClassifier(oob_score=True)\n",
    "    clf_full.fit(normed_data_full, y)\n",
    "\n",
    "    clf_final=RandomForestClassifier(oob_score=True)\n",
    "    clf_final.fit(normed_data_final, y)\n",
    "\n",
    "\n",
    "\n",
    "    test_features_full = feature_cols + [test_ycol]\n",
    "    test_features_final = final_features + [test_ycol]\n",
    "#     mci_datasets = [{'key' : 'ALL', 'data' : ADM_MCI[test_features].dropna()}, \n",
    "#                     {'key' : 'AB+', 'data' : ADM_MCI[ADM_MCI[\"ABP\"] == 1][test_features].dropna()},\n",
    "#                     {'key' : 'AB-', 'data' : ADM_MCI[ADM_MCI[\"ABP\"] == 0][test_features].dropna()},\n",
    "#                    ]  \n",
    "#     for mci_dataset in mci_datasets:\n",
    "    mci_df = ADM_MCI[test_features_full].dropna()\n",
    "    mci_test_X = mci_df[feature_cols]\n",
    "    mci_test_X = encodeCatsAndNormalize(mci_test_X)\n",
    "    sc_mci = StandardScaler()\n",
    "    normed_data_mci_full = pd.DataFrame(sc_mci.fit_transform(mci_test_X), columns = mci_test_X.columns)\n",
    "    normed_data_mci_final = normed_data_mci_full[final_features]\n",
    "\n",
    "    MCI_abp_true = mci_df[test_ycol]\n",
    "\n",
    "    y_abp_pred_full = clf_full.predict(normed_data_mci_full)\n",
    "    y_abp_pred_final = clf_final.predict(normed_data_mci_final)\n",
    "    \n",
    "    \n",
    "    tb = mcnemar_table(y_target=MCI_abp_true, \n",
    "                   y_model1=y_abp_pred_full, \n",
    "                   y_model2=y_abp_pred_final)\n",
    "\n",
    "    print(tb)\n",
    "    chi2, p = mcnemar(ary=tb, corrected=True)\n",
    "    if(tb[0, 1] + tb[1, 0] >= 25) :    \n",
    "        chi2, p = mcnemar(ary=tb, exact=True)\n",
    "        \n",
    "    return chi2, p \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f7bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1662498036313,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "160px",
    "width": "240px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "209.4px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
